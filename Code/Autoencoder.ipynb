{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "insured-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integrated-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 13\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spiritual-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('..\\\\Data\\\\ze41_mol_desc_db_red.csv', header=0, sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attempted-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = data.columns\n",
    "X = data[col_names[3:]].astype('float32')\n",
    "y = data[col_names[2]].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "biblical-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = MinMaxScaler(feature_range=(0,1)).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "meaningful-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y, test_size=0.1, random_state=seed)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "hourly-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(keras.models.Model):\n",
    "    def __init__(self, latent_dim=5):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim   \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            keras.layers.Dense(150, activation='linear'),\n",
    "            keras.layers.LeakyReLU(),\n",
    "            keras.layers.Dense(latent_dim, activation='linear'),\n",
    "            keras.layers.LeakyReLU(),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            keras.layers.Dense(150, activation='linear'),\n",
    "            keras.layers.LeakyReLU(),\n",
    "            keras.layers.Dense(len(X_train.columns), activation='linear')\n",
    "        ])\n",
    "    \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ordered-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(latent_dim=63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "later-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "conditional-permission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2313 - val_loss: 0.1722\n",
      "Epoch 2/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2009 - val_loss: 0.1378\n",
      "Epoch 3/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1544 - val_loss: 0.1059\n",
      "Epoch 4/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1124 - val_loss: 0.0800\n",
      "Epoch 5/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0811 - val_loss: 0.0649\n",
      "Epoch 6/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0642 - val_loss: 0.0578\n",
      "Epoch 7/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0546\n",
      "Epoch 8/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0538 - val_loss: 0.0528\n",
      "Epoch 9/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0520 - val_loss: 0.0512\n",
      "Epoch 10/75\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0504 - val_loss: 0.0496\n",
      "Epoch 11/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0490 - val_loss: 0.0481\n",
      "Epoch 12/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0475 - val_loss: 0.0468\n",
      "Epoch 13/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0462 - val_loss: 0.0457\n",
      "Epoch 14/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0449 - val_loss: 0.0445\n",
      "Epoch 15/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0439 - val_loss: 0.0436\n",
      "Epoch 16/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0428 - val_loss: 0.0425\n",
      "Epoch 17/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0416 - val_loss: 0.0414\n",
      "Epoch 18/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0404 - val_loss: 0.0400\n",
      "Epoch 19/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0391 - val_loss: 0.0388\n",
      "Epoch 20/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0377 - val_loss: 0.0374\n",
      "Epoch 21/75\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.036 - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0361\n",
      "Epoch 22/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0351 - val_loss: 0.0350\n",
      "Epoch 23/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0338 - val_loss: 0.0338\n",
      "Epoch 24/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0325 - val_loss: 0.0330\n",
      "Epoch 25/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0314 - val_loss: 0.0320\n",
      "Epoch 26/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0304 - val_loss: 0.0314\n",
      "Epoch 27/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0295 - val_loss: 0.0306\n",
      "Epoch 28/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0287 - val_loss: 0.0301\n",
      "Epoch 29/75\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0279 - val_loss: 0.0296\n",
      "Epoch 30/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0272 - val_loss: 0.0291\n",
      "Epoch 31/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0266 - val_loss: 0.0288\n",
      "Epoch 32/75\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.028 - 0s 8ms/step - loss: 0.0261 - val_loss: 0.0284\n",
      "Epoch 33/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0257 - val_loss: 0.0281\n",
      "Epoch 34/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0280\n",
      "Epoch 35/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0248 - val_loss: 0.0277\n",
      "Epoch 36/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0245 - val_loss: 0.0276\n",
      "Epoch 37/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0241 - val_loss: 0.0275\n",
      "Epoch 38/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0238 - val_loss: 0.0273\n",
      "Epoch 39/75\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.022 - 0s 8ms/step - loss: 0.0235 - val_loss: 0.0274\n",
      "Epoch 40/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0232 - val_loss: 0.0273\n",
      "Epoch 41/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0228 - val_loss: 0.0269\n",
      "Epoch 42/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0225 - val_loss: 0.0269\n",
      "Epoch 43/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0222 - val_loss: 0.0267\n",
      "Epoch 44/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0219 - val_loss: 0.0266\n",
      "Epoch 45/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0216 - val_loss: 0.0264\n",
      "Epoch 46/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0213 - val_loss: 0.0263\n",
      "Epoch 47/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0210 - val_loss: 0.0262\n",
      "Epoch 48/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0206 - val_loss: 0.0261\n",
      "Epoch 49/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0203 - val_loss: 0.0260\n",
      "Epoch 50/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0199 - val_loss: 0.0257\n",
      "Epoch 51/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0196 - val_loss: 0.0255\n",
      "Epoch 52/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0193 - val_loss: 0.0253\n",
      "Epoch 53/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0190 - val_loss: 0.0252\n",
      "Epoch 54/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0186 - val_loss: 0.0250\n",
      "Epoch 55/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0183 - val_loss: 0.0250\n",
      "Epoch 56/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.0249\n",
      "Epoch 57/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0176 - val_loss: 0.0247\n",
      "Epoch 58/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0173 - val_loss: 0.0246\n",
      "Epoch 59/75\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.017 - 0s 8ms/step - loss: 0.0170 - val_loss: 0.0243\n",
      "Epoch 60/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0166 - val_loss: 0.0242\n",
      "Epoch 61/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0163 - val_loss: 0.0241\n",
      "Epoch 62/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0160 - val_loss: 0.0240\n",
      "Epoch 63/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0158 - val_loss: 0.0240\n",
      "Epoch 64/75\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.015 - 0s 8ms/step - loss: 0.0155 - val_loss: 0.0240\n",
      "Epoch 65/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0153 - val_loss: 0.0237\n",
      "Epoch 66/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0149 - val_loss: 0.0235\n",
      "Epoch 67/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0147 - val_loss: 0.0232\n",
      "Epoch 68/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0144 - val_loss: 0.0231\n",
      "Epoch 69/75\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0141 - val_loss: 0.0229\n",
      "Epoch 70/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0138 - val_loss: 0.0229\n",
      "Epoch 71/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0228\n",
      "Epoch 72/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0134 - val_loss: 0.0229\n",
      "Epoch 73/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0133 - val_loss: 0.0226\n",
      "Epoch 74/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0130 - val_loss: 0.0232\n",
      "Epoch 75/75\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0131 - val_loss: 0.0225\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(X_train, X_train, validation_data=(X_valid, X_valid), epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "medical-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = autoencoder.encoder(X_train.to_numpy()).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "czech-compensation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.1270185 , -0.6010805 , -0.61468315, -1.0542951 , -0.59032327,\n",
       "        -0.4601769 ,  0.30387294,  0.03831662, -0.67637944, -0.29935572,\n",
       "        -0.12478977, -1.0921764 , -0.671218  , -1.3110518 , -0.25543138,\n",
       "        -0.20615743, -0.3036142 , -1.0204569 , -0.18818992, -0.62748957,\n",
       "        -0.95204514, -0.39997703, -0.22898522, -0.13860826, -0.06297753,\n",
       "        -1.0264585 , -0.800119  , -0.75881326,  2.0023322 , -0.76050377,\n",
       "        -0.1914937 , -0.94602823, -0.8972123 , -0.21145995, -0.9521343 ,\n",
       "        -0.8701328 , -0.15694396, -0.16717398, -0.447404  , -0.7705862 ,\n",
       "         0.19623733, -0.16675119, -1.041593  , -0.29204786,  0.19773118,\n",
       "        -0.4143301 , -0.22566482, -0.38851857, -0.23464409, -0.7208832 ,\n",
       "        -0.4243733 , -0.15748309, -0.49535978, -0.19403328,  0.15137514,\n",
       "        -0.27652723, -0.2279736 , -0.6726384 , -0.36468208, -0.7506586 ,\n",
       "        -0.02690163, -1.1793337 , -0.6645397 ], dtype=float32),\n",
       " array([-0.8039892 , -0.1679984 ,  0.7191011 , -0.24162932, -0.21119069,\n",
       "         0.06031073,  1.8543817 ,  1.2780446 , -0.3140677 ,  0.8132518 ,\n",
       "         0.48457962, -0.6257357 , -0.2153021 , -0.25605774,  0.40175322,\n",
       "         0.59878296,  0.14845726, -0.19387762,  0.5709308 , -0.25861806,\n",
       "        -0.44249073,  0.04382671,  0.50117975,  0.2859617 ,  1.0275096 ,\n",
       "        -0.588329  , -0.4448429 , -0.45257822,  3.4807496 , -0.42562908,\n",
       "         0.9156193 , -0.18169022, -0.4619755 ,  0.6165544 , -0.5021176 ,\n",
       "        -0.15292194,  1.2233667 ,  0.8437804 , -0.20707054,  0.17812318,\n",
       "         1.7148454 ,  1.4374102 , -0.5001227 ,  0.81008255,  1.6808059 ,\n",
       "        -0.04525836,  0.6087361 ,  0.9988196 ,  0.26509604, -0.4733418 ,\n",
       "        -0.17473939,  0.59460104,  0.01667581,  1.2051287 ,  1.9033055 ,\n",
       "         0.8177075 ,  0.22315013, -0.44071624,  0.36130986, -0.0076306 ,\n",
       "         0.7245388 , -0.7294085 ,  0.14874713], dtype=float32),\n",
       " array([-0.2754305 ,  0.23239873,  3.3182926 ,  0.9346818 ,  0.86850417,\n",
       "         1.6711179 ,  3.8929267 ,  2.6672378 ,  0.11691788,  1.9678756 ,\n",
       "         1.7742196 , -0.06151753,  0.25299087,  0.8809531 ,  1.766395  ,\n",
       "         1.4761176 ,  1.1513686 ,  0.73984754,  1.4224013 ,  0.4638184 ,\n",
       "        -0.18575008,  1.3204939 ,  1.791231  ,  1.6515657 ,  1.8805845 ,\n",
       "         0.14315133, -0.02173347, -0.0726442 ,  5.4526877 , -0.09135807,\n",
       "         3.389963  ,  1.0528557 , -0.08104186,  2.1867666 , -0.18909697,\n",
       "         1.4110252 ,  2.9049141 ,  3.2259674 ,  0.31312948,  1.5634377 ,\n",
       "         2.5987685 ,  2.472294  , -0.04367031,  1.6704037 ,  3.9926612 ,\n",
       "         0.81177616,  2.685933  ,  2.453795  ,  1.5936888 , -0.11429382,\n",
       "         0.6678951 ,  1.9561144 ,  1.5846522 ,  3.9295115 ,  3.8067157 ,\n",
       "         2.1017373 ,  1.1861248 , -0.16801426,  1.744926  ,  1.5800426 ,\n",
       "         2.0231671 , -0.24184707,  1.3742092 ], dtype=float32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(encoded, axis=0), np.mean(encoded, axis=0), np.max(encoded, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "prerequisite-lloyd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.all(np.isclose(encoded, 0), axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fuzzy-cruise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4169174 , -0.18855493, -0.64100254, -0.229885  , -0.33093712,\n",
       "       -0.1857523 ,  0.36452618, -0.26486614,  1.0547218 ,  0.96784663,\n",
       "       -0.33118272,  2.5293264 ,  0.8696953 , -0.01665194, -0.3071468 ,\n",
       "        1.5978403 , -0.86523074,  0.8250886 ,  0.83584434,  0.09430908,\n",
       "        0.7158391 , -0.14679645,  1.2907906 ,  0.58988464, -0.9638031 ,\n",
       "        1.4717908 , -0.21043524,  1.0412288 , -0.33622587,  1.4674118 ,\n",
       "       -0.95475924,  0.22502956,  1.2308071 , -0.32858747,  1.2273735 ,\n",
       "       -0.52595603,  0.8984458 , -0.16824324, -0.54150295, -0.5780772 ,\n",
       "        0.4244134 , -0.05069023,  0.06642459, -0.04441881, -0.10388943,\n",
       "       -0.6034974 , -0.67719936, -0.05721005,  1.4160762 , -0.46924374,\n",
       "       -0.51322293, -0.7776545 ,  2.9653423 ,  1.3085799 , -0.48950812,\n",
       "       -0.22778118, -0.29817116,  1.146699  , -0.9479503 , -0.24659275,\n",
       "       -0.64653736,  2.7394383 ,  2.3352356 ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "waiting-sleeve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-enlargement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
